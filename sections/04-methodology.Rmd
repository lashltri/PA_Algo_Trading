---
editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: inline
---

# Methodology

## Tools

Finally, it should be mentioned that generative AI systems and AI tools were used in
various phases of this project work. Specifically, ChatGPT was used to improve the
wording of the text and understand content, and to create program
code.

All analyses were implemented in the programming language R, using the following main packages: quantmod, PerformanceAnalytics, fGarch, xts, TTR, ggplot2, and dplyr. Code execution and development were carried out in RStudio.

Version control was managed using Git, with the complete source code archived in the associated GitHub repository.

The full source code of the project is available in the accompanying GitHub repository: https://github.com/lashltri/PA_Algo_Trading.git

## Data
Daily adjusted closing prices are obtained from Yahoo Finance using the quantmod package in R. The dataset covers a broad set of asset classes:

- **Equity indices** 

\begin{table}[H]
\centering
\caption{Equity indices and their listing currencies.}
\label{tab:equity}
\begin{tabular}{lll}
\toprule
\textbf{Ticker} & \textbf{Underlying}      & \textbf{Currency} \\
\midrule
SSMI            & Swiss Market Index       & CHF \\
GSPC            & S\&P 500                 & USD \\
IXIC            & NASDAQ 100               & USD \\
GDAXI           & DAX                      & EUR \\
\bottomrule
\end{tabular}
\end{table}

- **Energy commodities**

\begin{table}[H]
\centering
\caption{Energy commodities included in the dataset.}
\label{tab:energy}
\begin{tabular}{lll}
\toprule
\textbf{Ticker} & \textbf{Underlying} & \textbf{Currency} \\
\midrule
CL=F            & Crude Oil           & USD \\
NG=F            & Natural Gas         & USD \\
\bottomrule
\end{tabular}
\end{table}


- **Metals**

\begin{table}[H]
\centering
\caption{Metal commodities included in the dataset.}
\label{tab:metals}
\begin{tabular}{lll}
\toprule
\textbf{Ticker} & \textbf{Underlying} & \textbf{Currency} \\
\midrule
GC=F            & Gold                & USD \\
SI=F            & Silver              & USD \\
HG=F            & Copper              & USD \\
\bottomrule
\end{tabular}
\end{table}


- **Cryptocurrency**

\begin{table}[H]
\centering
\caption{Cryptocurrency included in the dataset.}
\label{tab:crypto}
\begin{tabular}{lll}
\toprule
\textbf{Ticker} & \textbf{Underlying} & \textbf{Currency} \\
\midrule
BTC-USD         & Bitcoin             & USD \\
\bottomrule
\end{tabular}
\end{table}

The “=F” suffix indicates that the price series refers to the corresponding futures contract on Yahoo Finance.

These assets were chosen to represent different market behaviours, volatility structures, and economic drivers.

### Sample Periods
Two separate sample periods are used in this thesis:

**Main sample:** 2014-2024
Used for the primary strategy evaluation across all assets.

**Crisis sample:** 2000-2009
Used separately to examine strategy behaviour during the global financial crisis.

**Why the main sample begins in 2014**

Although many assets in the dataset have data available long before 2014, Bitcoin (BTC-USD) only begins in Yahoo Finance in 2014. Since the strategy framework relies on a five-year rolling window for parameter tuning, at least five years of historical data are required before the first out-of-sample signal can be generated.
To ensure a common evaluation period across all assets, the main sample therefore begins in 2014.

**Why Bitcoin is excluded from the crisis sample**

Bitcoin did not exist during 2000–2009, and no reliable historical data is available. As a result, Bitcoin is excluded from the crisis analysis, which focuses only on assets with complete coverage in that period.

### Data Cleaning and Preprocessing
Missing values are handled via linear interpolation with a maximum gap of five consecutive days, following the implementation in the project code.

For the few instances where adjusted prices were negative, values are replaced with a small positive constant (0.01) to avoid undefined log-returns. The affected observations are minimal and do not influence long-term behaviour.

All price series are converted to log-returns, which serve as the basis for signal generation and backtesting. Conversion to simple returns is performed later in the methodology section when required for performance metrics such as the Sharpe ratio.

\newpage

### Distributional Diagnostics of Log-Returns
Across all assets, the empirical return distributions deviate from normality and show fat tails. This is typical for financial data. This can be seen in Figure \@ref(fig:qqplotsnormal):

```{r qqplotsnormal, fig.cap="QQ-plots comparing log-returns to a normal distribution.", echo=FALSE, message=FALSE, fig.width=7, fig.height=7 }


# 1) Build list of log-returns as plain numeric vectors
ret_list <- lapply(px_list, function(x) {
  r_xts <- diff(log(Ad(x)))     # log-returns as xts
  as.numeric(r_xts)             # drop xts class, keep numeric only
})
names(ret_list) <- names(px_list)   # keep the asset names

# 2) Set up plotting grid (10 assets -> 2 x 5)
old_par <- par(mfrow = c(4, 3))

# 3) QQ-plots vs Normal distribution
for (nm in names(ret_list)) {
  qqPlot(
    ret_list[[nm]],
    distribution = "norm",
    main = paste("Normal QQ:", nm),
    ylab = "Log-Returns",
    xlab = "Normal quantiles",
    id = FALSE           # <--- no text labels inside the panel
  )
}

# 4) Restore graphics parameters
par(old_par)


```
\newpage
To check whether a heavier-tailed distribution provides a better fit, a Student-t distribution was also tested. The QQ-plots indicate that a Student-t distribution offers a more reasonable match to the empirical data (Figure \@ref(fig:qqplotst)). While the exact degrees of freedom would need to be estimated separately for each asset, this step is not required here, as the purpose of the diagnostic is only to ensure that a heavy-tailed distributional assumption is reasonable.
```{r qqplotst, fig.cap = "QQ-plots comparing log-returns to a Student-t distribution.", echo=FALSE, message=FALSE , fig.width=7, fig.height=7}

# 1) Build list of log-returns as plain numeric vectors
ret_list <- lapply(px_list, function(x) {
  r_xts <- diff(log(Ad(x)))     # log-returns as xts
  as.numeric(r_xts)             # drop xts class, keep numeric only
})
names(ret_list) <- names(px_list)

# 2) Set up plotting grid
old_par <- par(mfrow = c(4, 3))

# 3) QQ-plots vs Student t distribution (choose df, e.g. 5)
df_t <- 4

for (nm in names(ret_list)) {
  qqPlot(
    ret_list[[nm]],
    distribution = "t",    # <- Student t
    df = df_t,             # <- degrees of freedom
    main = paste("Student-t QQ:", nm),
    ylab = "Log-Returns",
    xlab = paste0("t-quantiles (df = ", df_t, ")"),
    id = FALSE
  )
}

# 4) Restore graphics parameters
par(old_par)

```

\newpage

## Pipeline

![Software pipeline](sections/images/pipeline.png){#fig:pipeline-fig width="50%"}

Figure \@ref(fig:pipeline-fig) illustrates the analysis pipeline
implemented in this project. The process begins with the import of
historical market data from the Yahoo Finance API. The raw data is
cleaned, aligned across assets, and missing values are approximated when
necessary. Overall, only minimal preprocessing is required, as the data
is already of high quality.

After preprocessing, trading strategies are constructed. Alongside
traditional technical strategies such as Moving Averages and Bollinger
Bands, an ARMA-GARCH model is fitted to capture return dynamics and
volatility clustering. The conditional volatility estimates obtained
from the GARCH component are used for two separate purposes. Within the
ARMA-GARCH strategy, they determine dynamic position sizing, and for all
strategies, they are used to standardize returns prior to hypothesis
testing, reducing heteroscedasticity and creating approximately
independent data.

The strategies are then executed in an out-of-sample backtesting
environment where the estimated log-returns of the strategies are
returned.

Finally, a paired hypothesis test is performed to evaluate whether
differences in performance are statistically significant.

## Trading-Strategies

### Moving Average Strategy

The equal-weighted Moving Average (EqMA) (see Equation \@ref(eq:sma)) is
applied to the adjusted closing prices of the assets. Two different
window lengths are defined, a short-term window $N_1$ and a long-term
window $N_2$. The parameters are determined through hyperparameter
tuning based on in-sample performance (see Chapter
\@ref(sec:parametertuning)).

The trading signals follow the crossover rule. This is a common trading rule is based on the crossing of a short-term and a long-term moving average [@wildislidestradingindicators].\
$N_1$ represents the short-term window and $N_2$ the long-term window, with $N_1 < N_2$.\
The trading position $POS_{t+1}$ at time $t$ is defined as:

\begin{equation}\label{eq:ma_crossover}
\mathrm{POS}_{t+1} =
\begin{cases}
1,  & \text{if } MA_t(N_1) > MA_t(N_2) \ \text{(long)} \\[6pt]
0, & \text{if } MA_t(N_1) < MA_t(N_2) \ \text{(flat)} \\[6pt]
0,  & \text{if } MA_t(N_1) = MA_t(N_2) \ \text{(flat)}
\end{cases}
\end{equation}

The following MA function defined in Example \@ref(exm:mafun) is applied
to each stock index. The paramerters $N_1$ and $N_2$ are tuned as
described in Chapter \@ref(sec:parametertuning), the optimal parameters
are determined annually based on a rolling five-year window.

::: {#exm:mafun .example}
\label{exm:mafun} MA function in R

```{r, ref.label='strat_ma-fn', echo=TRUE, eval=FALSE}

```
:::

**Example implementation on the SSMI**

Figure \@ref(fig:maStrategy) is an implementation of the Moving Average
(MA) crossover strategy on the SMI for the period 2019–2024.\
The short-term (red) and long-term (blue) moving averages determine the
trading positions according to the crossover logic. The grey bars in the
lower panel represent the trading signals generated.

```{r maStrategy, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="SSMI with short-term (red) and long-term (blue) moving averages and corresponding trading signal (2019–2024). Parameters $N_1$ and $N_2$ are estimated annually using a rolling five-year window.", out.width="80%", fig.align="center", fig.keep="last", fig.width=9, fig.height=7, dpi=300}



SSMI <- px_list$SSMI$SSMI.Adjusted
SMA_N1 <- STRATS$MA$SSMI$strategy$`SMA short`
SMA_N2 <- STRATS$MA$SSMI$strategy$`SMA long`
signal <- STRATS$MA$SSMI$signal
from_to<-"2019-01::2024-12-30"

chartSeries(SSMI,
            type = "line",
            subset=from_to,
            theme=chartTheme("white"))
addTA(SMA_N1,col="red", on = 1)
addTA(SMA_N2,col="blue", on = 1)
addTA(signal,col="grey", type = "h")

# chart_Series(SSMI, name = "SSMI MA Strategy")
# add_TA(SMA_N1, col = "red",  on = 1, lwd = 2)
# add_TA(SMA_N2, col = "blue", on = 1, lwd = 2)
# add_TA(signal,  col = "grey", type = "h", on = NA, lwd = 1)
```
 \newpage
### Bollinger Bands

The Bollinger Bands strategy uses the indicator introduced in Section @ref(sec:bbtheory) and applies it to the adjusted closing prices of each asset. Two parameters determinecthe bands: the window length $N$ for the rolling mean and standard deviation, and the width multiplier $k$. Both parameters are determined through hyperparameter tuning based on in-sample performance (see Section @ref(sec:parametertuning)). As with the Moving Average strategy, the parameters are re-estimated annually using a rolling five-year training window.

The trading signals are based on the breakout interpretation of Bollinger Bands [@wildislidestradingindicators]. This mehtod does not require mean reversion.

Let $x_t$ is defined at the adjusted closing price, and $U_t$ and $L_t$ as the upper and lower bands computed at time $t$.

The trading position at time $t+1$ is defined as:
\begin{equation}\label{eq:bbsignal}
\mathrm{POS}_{t+1} =
\begin{cases}
1, & \text{if } x_t > U_t \quad \text{(long)} \\[6pt]
0, & \text{if } x_t < L_t \quad \text{(flat)} \\[6pt]
\end{cases}
\end{equation}

The full strategy function, is documented in the Appendix Section \@ref(sec:appendixB), as the imlementation itself is not trivial.

**Volatility estimate**

Although the rolling standard deviation (Equation \@ref(eq:rollingvola)) used in Bollinger Bands could in principle be replaced by a GARCH-based volatility estimate, this was intentionally not done. First, for reasons of fairness and methodological separation, the ARMA-GARCH specification appears in this thesis as an independent strategy. Embedding a GARCH model inside the Bollinger Band indicator would blur this separation and partially turn the BB strategy into another volatility based approach. Second, using GARCH-based volatility for all filter strategies would make them more similar in their reaction to volatility clustering, reducing the performance contrast between the different methods. This could weaken the ability to detect meaningful differences across strategies.

**Example implementation on the SSMI**

Figure @ref(fig:bbStrategy) shows the implementation of the Bollinger Bands (BB) breakout strategy on the SSMI for the period 2018–2021. The sample window is a window chosen for visiuallisation purposes only.
The middle band (blue), together with the upper and lower bands (grey dashed) are used for the trading decisions.
A long position is entered whenever the price breaks above the upper band, and the strategy switches back to a flat position when the price falls below the lower band.
The grey bars in the lower panel show the resulting trading signals generated by this breakout logic.
```{r bbStrategy, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="SSMI with optimized Bollinger Bands (blue = moving average, grey = ± $k * sigma$) and trading signal (2018–2021). Parameters $N$ and $k$ are estimated annually using a rolling five-year window.", out.width="80%", fig.align="center", fig.keep="last", fig.width=9, fig.height=7, dpi=300}



SMA <- STRATS$BB$SSMI$strategy$mavg
up <- STRATS$BB$SSMI$strategy$up
dn <- STRATS$BB$SSMI$strategy$dn
signal <- STRATS$BB$SSMI$signal
from_to<-"2019-01-01::2021-01-01"

chartSeries(px_list$SSMI$SSMI.Adjusted,
            type = "line",
            subset=from_to,
            theme=chartTheme("white"), name = "SSMI BB Strategy")

addTA(SMA,type="S",col="blue", on = 1)
addTA(up,type="S",col="darkgrey", on = 1, lty = 2)
addTA(dn,type="S",col="darkgrey", on = 1, lty = 2)
addTA(signal,col="grey", type = "h")



```
\newpage
### Moving Average Convergence Divergence
The MACD strategy is constructed in Section \@ref(sec:macdtheory) to the adjusted closing prices of each asset. The parameters are $(S,L,R)$, where $S$ is the fast EMA length, $L$ the slow EMA length, and $R$ the signal-line EMA length. These parameters are treated as hyperparameters and are selected through the tuning procedure described in Section \@ref(sec:parametertuning).

The trading rule is based on the crossover of the MACD line and its
signal line. 
The trading position at time $t+1$ is defined as:
\begin{equation}\label{eq:macd_crossover}
\mathrm{POS}_{t} =
\begin{cases}
1,  & \text{if } \mathrm{MACD}_t > \mathrm{Signal}_t \quad \text{(long)} \\[6pt]
0,  & \text{if } \mathrm{MACD}_t < \mathrm{Signal}_t \quad \text{(flat)} \\[6pt]
\end{cases}
\end{equation}

The following MACD function defined in Example \@ref(exm:macdfun) is applied to each stock index. The parameters are tuned as described in Section \@ref(sec:parametertuning) and determined annually based on a rolling five-year window.

::: {#exm:macdfun .example}
\label{exm:macdfun} MACD function in R

```{r, ref.label='strat_macd-fn', echo=TRUE, eval=FALSE}

```
:::

**Example implementation on the SSMI**

Figure \@ref(fig:macdStrategy) shows the implementation of the MACD strategy on the SSMI ticker for the period of 2020. The sample window is chosen for visualisation purposes only and does not correspond to the full backtesting period.

The MACD line (blue), the signal line (red), and the MACD histogram (grey bars) are shown in the middle panel.
A long position is taken whenever the MACD line crosses above the signal line, and the strategy returns to a flat position when the MACD line drops below the signal line.
The grey bars in the lower panel display the resulting trading signals generated by this crossover logic.
```{r macdStrategy, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = "SSMI with optimized MACD indicator (blue = MACD line, red = signal line, grey = histogram) and resulting trading signal (2020). Parameters $S$, $L$, and $R$ are estimated annually using a rolling five-year window.", out.width="80%", fig.align="center", fig.keep="last", fig.width=9, fig.height=7, dpi=300}
from_to <- "2020-01-01::2020-12-30"

price   <- STRATS$MACD$SSMI$price
signal  <- STRATS$MACD$SSMI$signal

macd_line   <- STRATS$MACD$SSMI$strategy$macd
signal_line <- STRATS$MACD$SSMI$strategy$signal
macd_hist   <- macd_line - signal_line

chartSeries(px_list$SSMI$SSMI.Adjusted,
            type   = "line",
            subset = from_to,
            theme  = chartTheme("white"),
            name   = "SSMI Strategy")

## 1) MACD indicator in panel 2
addTA(macd_line,   type = "l", col = "blue", on = NA, legend = NULL)  # creates panel 2
addTA(signal_line, type = "l", col = "red",  on = 2)   # same panel as MACD
addTA(macd_hist,      type = "h", col = "darkgrey", on = 2)


## 2) Trading signals in panel 3
addTA(signal, type = "h", col = "grey", on = NA)
```


\newpage

## ARMA - GARCH

This section describes how ARMA–GARCH forecasts are used to generate
trading signals. The statistical model and its theoretical properties
are discussed in Section \@ref(theoryarmagarch) Here, the focus is on
the practical implementation.

### Identification (in-sample) {#estimGARCH}

The ARMA–GARCH models are estimated separately for each index. No
automated model selection or parameter tuning is applied.

The in-Sample Period is defined as the beginning of each asset history
(2014 or later) up to 2019. This ensures consistency with
other trading strategies (same cut-off date) and that the estimation
window is sufficiently long even for assents with shorter histories such
as Bitcoin.

The ARMA orders may differ across assets, based on Bayesian information
criteria (BIC) and the autocorrelation structure. The GARCH(1,1)
specification is kept constant, since it is widely used in financial
volatility modeling and provides a robust baseline for risk forecasting.
The innovation distributions were also varied during the estimation
process, the Student-t distribution was always preferable to the
Gaussian distribution .

Models are estimated via maximum likelihood.

Models were assessed based on residual diagnostics as discussed in
Section \@ref(theoryarmagarch), including Ljung-Box statistics. Only
models that passed these diagnostics were considered suitable for
forecasting. Among the valid candidates for each asset, the final model
specification was selected by minimizing the Bayesian Information
Criterion (BIC), ensuring a balance between goodness of fit and model
parsimony.

The resulting specifications for the assets are summarized in following
Table \@ref(tab:modelspec):

```{r modelspec, echo=FALSE, results='asis'}
library(knitr)

modelspec <- data.frame(
  Asset = c("SSMI", "GSPC", "IXIC", "GDAXI", "BTC-USD", "CL=F", "GC=F", "NG=F", "SI=F", "HG=F"),
  Model = c("ARMA(1,0)-GARCH(1,1)",
            "ARMA(0,1)-GARCH(1,1)",
            "ARMA(0,1)-GARCH(1,1)",
            "ARMA(1,0)-GARCH(1,1)",
            "ARMA(1,1)-GARCH(1,1)",
            "ARMA(1,0)-GARCH(1,1)",
            "ARMA(1,0)-GARCH(1,1)",
            "ARMA(1,0)-GARCH(1,1)",
            "ARMA(1,0)-GARCH(1,1)",
            "ARMA(1,0)-GARCH(1,1)")
)


kable(modelspec,
      caption = "(\\#tab:modelspec) Model specifications",
      format = 'simple', align = "l")

```

**Example Model Identification SMI**

In this example the Daily log-returns of the SMI were to be molded using
an ARMA-GARCH model. The in-sample period begins in 2010 and ends after
2018. The remaining observations were reserved for later out-of-sample
forecasting and backtesting.

In a first step, the in-sample prices and corresponding log-returns were
visualized to obtain a general understanding of the market’s level and
volatility patterns. The first panel of Figure \@ref(fig:ssmiAGprice)
shows a steady upward trend until about 2015.

The second panel plots the corresponding log returns for the same
in-sample period. As expected the log return appear to be stationary,
but exhibit volatility clustering. These observations justify the use of
an ARMA-GARCH model.

Although the mean of returns does not differ significantly from zero, it
was observed empirically that including a mean term (μ) leads to less
frequent switching of trading positions in the subsequent strategy
implementation. This behavior is preferred, therefore the mean component
was estimated for all assets, even in cases where statistical criteria
suggested it was not strictly necessary.

```{r ssmiAGprice, echo = FALSE, fig.cap= "In-sample SMI price and corresponding log-returns (2010–2019).", out.width="70%"}
  price <- px_list$SSMI$SSMI.Adjusted
  ret <- na.omit((suppressWarnings(diff(log(price)))))

  # In and out of sample definieren
  in_sample<-min(common_idx) #we want to then be out of sample at min(common_idx)
  
  ret_out<-ret[paste(in_sample,"/",sep="")]
  ret_in  <- ret[!index(ret) %in% index(ret_out)]
  
  par(mfrow = c(2,1))
  plot(price[index(ret_in)], main = "In Sample SMI Price")
  plot(ret_in, main = "In Sample SMI Log-Returns")
```

After defining the in-sample log-returns of the SMI, the next step was
to identify a suitable ARMA-GARCH specification for modelling both the
conditional mean and variance dynamics of returns.

Several ARMA–GARCH models were fitted to the in-sample returns, the
ARMA(1,1)–GARCH(1,1) with normal errors serves as a great bench line and
model parameters were reduced from that point as well as the
distributional assumptions.

All models passed the Ljung-box residual diagnostics tests both for the
mean level equation and the variance equation. Only the distributional
assumptions were not sufficient, no matter the chosen distribution. This
is a common issue in financial time series modelling, and therefore not
considered critical here.

Both the AR(1)-GARCH(1,1) and the MA(1)-GARCH(1,1) specifications with
skewed-Student-t errors achieved similar BIC values; however, the AR(1)
version was preferred for its simpler interpretation and ease of
out-of-sample forecasting.

::: example
SMI Model Esimation

```{r, echo=TRUE, eval=FALSE}
  fit<-garchFit(~arma(1,1)+garch(1,1),data=ret_in,delta=2,
                include.delta=F,include.mean=T, cond.dist = "norm")
  s<-summary(fit)
  s$stat_tests #All diagnostics okay, except for distributional assumptions
  s$ics #BIC = -6.653617
  
  #---
  fit2<-garchFit(~arma(1,1)+garch(1,1),data=ret_in,delta=2,
                include.delta=F,include.mean=T, cond.dist = "sstd")
  s2<-summary(fit2)
  s2$stat_tests #All diagnostics okay, except for distributional assumptions
  s2$ics #BIC = -6.700768 
  
  #---
  fit3<-garchFit(~arma(0,1)+garch(1,1),data=ret_in,delta=2,
                 include.delta=F,include.mean=T,  cond.dist = "sstd")
  s3<-summary(fit3)
  s3$stat_tests #All diagnostics okay, except for distributional assumptions
  s3$ics #BIC =  -6.703647
  
  #---
  fit4<-garchFit(~arma(1,0)+garch(1,1),data=ret_in,delta=2,
                 include.delta=F,include.mean=T,  cond.dist = "sstd")
  s4<-summary(fit4)
  s4$stat_tests #All diagnostics okay, except for distributional assumptions
  s4$ics #BIC = -6.703622
```
:::

### Estimation (in-sample)

In the next step, the parameters of the selected models are estimated
using data from the same in-sample period.

Re-estimating ARMA-GARCH on short rolling windows also tends to produce unstable or non-convergent parameter estimates and adds substantial computational overhead.

For these reasons, the ARMA-GARCH model is estimated once in a stable in-sample period and then applied consistently out-of-sample. Only the simpler technical indicators are tuned in a rolling-window framework because their parameters directly define the trading rules and behave reliably under this type of optimization.

### Forecasting and Backtesting (out-of-sample)

During the out-of-sample period (2019-2025), one-step-ahead forecasts of
both the mean return and the conditional volatility are generated
recursively based on the estimated in-sample parameters and formulas for
mean return forecasts (formula \@ref(eq:armamean) and conditional
volatility forecasts (formula \@ref(eq:garchvariance). Because price
levels $x_t$ are mostly non-stationary, models are estimated with
log-returns: $y_t = log(x_t)-log(x_{t-1})$

A long-only, volatility-scaled trading strategy is constructed from
these forecasts:

1.  **Position sizing\
    **$$
    w_t = \frac{1}{\sigma_t},
    \qquad \mathbb{E}[w_t] = 1
    $$

    The position size decreases during periods of high predicted
    volatility and increases when volatility is expected to be low. The
    position sizes are then normalized so that their expected value
    equals one, which implicitly allows for leveraging.

2.  **exposure_t\
    **$$
    \text{exposure}_t =
    \begin{cases}
    w_t, & \hat{y}_t > 0, \\[4pt]
    0, & \text{otherwise}
    \end{cases}
    $$**\
    **Positive expected returns lead to a long position, while negative
    or zero forecasts result in a flat position.**\
    **

3.  **Strategy returns\
    **$$
    r_t = \text{exposure}_t \cdot y_t
    $$

The same modelling and trading procedure is applied consistently across
all assets, which enables a fair comparison of performance across
markets.

The implemented R function for the ARMA-GARCH function can be found in
the Appendix \@ref(appendixA)

**Example implementation on the SMI**

The figure \@ref(fig:AGStrategy) shows the ARIMA-GARCH implementation
using the SMI as the underlying asset. As visible in the visualization,
volatility tends to increase during market downturns. For example, in
2020 the sharp decline in prices is accompanied by elevated volatility,
and the trading strategy responds by reducing position sizes
accordingly.

```{r AGStrategy, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="SMI ARMA-GARCH Strategy with GARCH Estimated Volatility and Trading Signal", out.width="90%", fig.align="center", fig.keep="last", fig.width=9, fig.height=7, dpi=300, results='hide'}



dat <- strat_ag_arma_garch(price = px_list$SSMI$SSMI.Adjusted, 
                           model = "arma(1,0) + garch(1,1)",
                           return_strat = T)

chartSeries(dat$price,
            type = "line",
            subset=date_from_to,
            theme=chartTheme("white"), name = "SSMI ARMA-GARCH Strategy")
addTA(dat$vola,col="red", type = "l", legend = "Estimated GARCH Volatility")
addTA(dat$signal,col="grey", type = "h", legend = "Position Size (Signal)")




```

\newpage
## Backtesting

### Parameter Tuning {#sec:parametertuning}

The purpose of the tuning step is to identify, for each strategy, the
set of parameters that leads to the best in-sample performance, before
being applied out-of-sample.

For every strategy, a predefined parameter grid is evaluated within a
rolling-window framework:

$$
\begin{aligned}
\text{Moving Average} \quad & N_1 \in \{10, 20, 30, 40, 50\}, 
\quad N_2 \in \{80, 100, 150, 200, 250\} \\
\text{Bollinger Bands} \quad & N \in \{10, 20, 30, 40\}, 
\quad K \in \{1.5, 2, 2.5, 3\} \\
\text{MACD} \quad & S \in \{6, 8, 10, 12, 15\}, 
\quad L \in \{18, 20, 26, 30, 35\}, 
\quad R \in \{9, 10, 12\}
\end{aligned}
$$

All possible parameter combinations from these grids are tested inside
each training window.

The tuning follows a rolling window approach. Each iteration uses five
years of historical data as a training set. Within that window, all
possible parameter combinations from the grid are evaluated, and the one
that performs best is selected. This chosen configuration is then used
for the following year, the test year, before the window shifts forward
by one year and the process repeats.

The rolling window was chosen because it accounts for changing market
conditions over time. Parameters of financial data is rarely constant,
and estimates based on the full sample size or an expanding window would
ignore structural shifts. Re-estimating parameters over using
overlapping samples allows the model to adapt to these
changes.[@ModelingFinancialTimeSeries]

In addition, a lookback period is added before each test year to provide
the historical data required by indicators such as moving averages or
Bollinger Bands. This short “warm-up” phase overlaps with the end of the
training window. This overlap does not create information leakage or
bias; the lookback simply provides the historical context needed to
initialize indicators, just as a real trader would rely on recent data
at the start of a new period. The length of this "warm-up" period
depends on the Parameters and specific needs of the Strategy.

Performance during the tuning is measured by the annualized Sharpe
ratio. The Sharpe ratio is used here because it has the same structure
as the later test statistic, differing only in the volatility-clustering
scaling of the returns.

The Following Graphic (\@ref(fig:figrollingwindow)) is a visual aid of Rolling Window, it is an
example not the exaxt implementation as the dates dont line up for ease
of interpretation reasons and the "warm-up" period is also just a fixed
length example.

```{r figrollingwindow, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Example of Rolling Window Tuning. This figure is a simplified visual aid for interpretation, dates and warm-up lengths are illustrative only.", out.width="60%", fig.align="center", fig.pos='H'}

library(ggplot2)
library(dplyr)

# Years 2005–2015
yrs <- 2005:2015

# Window lengths
train_len <- 5
test_len  <- 1

# Lookback (Warm-up)
max_lb_days <- 200
trading_days_per_year <- 252
lb_years <- max_lb_days / trading_days_per_year  # fraction of a year

# Iterations
n_iter <- length(yrs) - train_len
if (n_iter < 1) stop("Need at least 6 years between 2005–2015.")

rows <- tibble(
  iter        = 1:n_iter,
  train_start = yrs[1:n_iter],
  train_end   = yrs[train_len:(train_len + n_iter - 1)],
  test_start  = yrs[(train_len + 1):(train_len + n_iter)],
  test_end    = yrs[(train_len + 1):(train_len + n_iter)] + test_len - 1
)

# Overlap (Warm-up)
overlap_start <- pmax(rows$test_start - lb_years, rows$train_start)
overlap_end   <- rows$test_start

# Main data blocks
df_train_left <- rows |>
  transmute(iter, type = "Training",
            xmin = train_start, xmax = overlap_start,
            ymin = iter - 0.35, ymax = iter + 0.35)

df_overlap_train <- rows |>
  transmute(iter, type = "Training (overlap)",
            xmin = overlap_start, xmax = overlap_end,
            ymin = iter, ymax = iter + 0.35)

df_overlap_warm <- rows |>
  transmute(iter, type = "Warm-up (lookback)",
            xmin = overlap_start, xmax = overlap_end,
            ymin = iter - 0.35, ymax = iter)

df_test <- rows |>
  transmute(iter, type = "Test",
            xmin = test_start, xmax = test_end + 1,
            ymin = iter - 0.35, ymax = iter + 0.35)

# Colors
cols <- c(
  "Training"            = "#2E86DE",
  "Training (overlap)"  = "#2E86DE",
  "Warm-up (lookback)"  = "#00E5FF",
  "Test"                = "#E67E22"
)

# Labels
lab_train <- rows |>
  transmute(iter,
            x = (train_start + train_end + 1) / 2,
            y = iter,
            txt = paste(train_start, "-", train_end))

lab_test <- rows |>
  transmute(iter,
            x = (test_start + test_end + 1) / 2,
            y = iter,
            txt = (test_start))

ggplot() +
  geom_rect(data = df_test,
            aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = type)) +
  geom_rect(data = df_train_left,
            aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = type)) +
  geom_rect(data = df_overlap_warm,
            aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = type), alpha = 0.85) +
  geom_rect(data = df_overlap_train,
            aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = type)) +
  geom_text(data = lab_train, aes(x = x, y = y, label = txt), color = "white", size = 3, fontface = "bold") +
  geom_text(data = lab_test,  aes(x = x, y = y, label = txt), size = 3, fontface = "bold") +
  scale_fill_manual(
    values = cols,
    breaks = c("Training", "Test", "Warm-up (lookback)"),   
    labels = c("Training", "Test", "Warm-up"),
    guide = guide_legend(override.aes = list(alpha = 1))    
  ) +
  scale_y_reverse(breaks = rows$iter) +
  scale_x_continuous(breaks = 2005:2015, limits = c(2005, 2016)) +
  labs(
    x = "Year",
    y = "Iteration",
    fill = NULL
  ) +
  theme_minimal(base_size = 12) +
  theme(
    panel.grid.minor = element_blank(),
    axis.ticks.y = element_blank(),
    legend.position = "top",
    legend.direction = "horizontal",
    legend.title = element_blank()
  )

```


### Strategy Returns

Trading positions are signals $POS_t \in \{0, 1\}$ generated by trading
strategies.\
Daily log-returns are computed as:

\begin{equation}\label{eq:logret}
r_t = \log\!\left(\frac{P_t}{P_{t-1}}\right) \cdot POS_t
\end{equation}

$P_t$ is the adjusted closing price at time $t$.\
These returns serve as the input for the evaluation of strategy
performance.

::: example
\label{exm:backtest} Strategy log-returns function in R

```{r, ref.label='backtest_logRet-fn', echo=TRUE, eval=FALSE}

```
:::

### Alignment of Trading Days
Because the assets follow different trading calendars, the resulting return series do not share exactly the same dates. After computing strategy returns, all series are therefore restricted to the intersection of their trading days. This is implemented by intersecting the date indices of all backtested return series and subsetting each series to this common index. As a result, all strategies are evaluated on an identical set of dates.

\newpage
## Portfolio Construction

After generating individual strategy returns for each individual asset,
performance should also be assessed on an overall portfolio. This
portfolio is created in a next step.

$R_{i,t}$ references simple returns, with $i$ corresponding to the asset
and $t$ the time.\
The portfolio return is calculated as the equal-weighted average of all
simple returns:

$$
R_t^P = \frac{1}{N} \sum_{i=1}^{N} R_{i,t}
$$

The implementation in R is as follows:

::: example
\label{exm:portfolio} Portfolio construction function in R

```{r, ref.label='create_portfolio-fn', echo=TRUE, eval=FALSE}

```
:::

The reasoning behind using an equally weighted portfolio, as opposed to
an optimized one such as in the Markowitz framework, is to maintain an
unbiased and transparent comparison across trading strategies.

The primary focus of this project lies in evaluating the effectiveness
of the trading rules themselves rather than optimizing asset allocation.
This approach ensures that each asset contributes equally to the
portfolio’s performance, independent of characteristics such as
volatility.

Weight optimization would distort the visibility of the strategies
characteristics and instead emphasize the performance of the underlying
assets rather than the strategies themselves.

\newpage

## Evaluation criteria and testing {#ttesting}
For comparability with performance measures such as the annualized Sharpe ratio, log-returns are converted to simple returns using:
$$R_{t} = e^{r_{t}}-1$$

The annualized Sharpe ratio is used as the primary evaluation metric because it provides a simple and comparable measure of performance across assets with very different volatility levels. The measure follows the definition given in Equation~\ref{eq:sharpe} in the Theory section and is computed with a risk-free rate of \(R_f = 0\), which is standard for daily data. In practice, the Sharpe ratio is calculated using the implementation provided by the PerformanceAnalytics package in R.

Maximum drawdown is not included as an additional criterion. With multiple strategies and asset categories already being compared, adding more evaluation dimensions would dilute the focus and make interpretation more difficult. The aim is to keep the comparison clear and centred on one consistent performance measure.



### Statistical Testing

To compare the performance of two trading strategies $A$ and $B$, a
paired $t$-test is performed on their standardized log-return series.
For financial return data, the test is valid when the difference series is approximately uncorrelated and exhibits stable variance. To meet these conditions, the returns of each strategy are first standardised by their conditional volatility estimates.
For each strategy $i \in \{A,B\}$, standardized returns are obtained by:

$$
z_t^{(i)} = \frac{r_t^{(i)}}{\hat\sigma_t^{(i)}}
$$ $r_t^{(i)}$ is the series of strategy returns (as defined in Equation
\@ref(eq:logret)) and $\hat\sigma_t^{(i)}$ the conditional standard
deviations. For all filter-based strategies with binary exposure
$POS_t \in \{0,1\}$, the volatility $\hat\sigma_t^{(i)}$ corresponds to
the ticker price returns, as these strategies simply switch market by
either being in or out and inherit the underlying asset’s risk during
periods in the market. The ARMA-GARCH strategy, which uses continuously
scaled position sizes $signal_t \in (0,1]$, is standardized using a
separate GARCH(1,1) model fitted directly to its own strategy returns to
ensure a consistent risk-adjusted comparison across all strategies. This 

The paired $t$-statistic is defined as:

$$
d_t = z_t^{(A)} - z_t^{(B)}
$$

\begin{equation} \label{eq:tstat}
t_{A,B} = \frac{\sqrt{n}\,\bar{d}}{s_d} \;\sim\; t_{n-1}
\end{equation}

$\bar{d}$ represents the mean of the differences and $s_d$ their
empirical standard deviation.

It is a nice coincidence that the structure of this test statistic is
similar to a Sharpe ratio. It can conceptually be viewed as a
Sharpe-type measure applied to the performance difference between the
two strategies, showing how one performs relative to the other on a
risk-adjusted basis.

The null hypothesis is defined as:

\begin{equation} \label{eq:H0}
H_0\!:\! \mathbb{E}[d_t] = 0
\end{equation}

This implies that both strategies yield equal mean standardized returns,
while the alternative hypothesis $H_A\!:\! \mathbb{E}[d_t] \neq 0$
suggests a statistically significant performance difference.

Since the returns are standardized using GARCH-based conditional
volatilities, heteroskedasticity is accounted for. No correction for
autocorrelation is applied, as the standardized daily returns can be
reasonably assumed to be approximately uncorrelated.

No formal correction for multiple testing is applied. Given that
financial return series typically resemble white noise, any significant
result under this framework is already somewhat unlikely. Applying
conservative corrections such as Bonferroni adjustments would contibute
to Type II errors and mask potentially meaningful differences. Thus,
significance levels are to be interpreted cautiously rather than as
definitive inferential evidence. [@hofer2023gstat]

